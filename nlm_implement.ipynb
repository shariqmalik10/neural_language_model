{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing libs \n",
    "import torch as torch\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt \n",
    "%matplotlib inline\n",
    "\n",
    "import time\n",
    "import random\n",
    "\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating dataset \n",
    "Here we will create the X and Y training sets using the dataset the user enters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_vocab(words):\n",
    "    # we are no longer going to have just 27 indices. our vocabulary size is a lot bigger since we are now working with word-level model not character level model .so each word is assigned an index \n",
    "\n",
    "    # # here i am getting the count for every word in the dataset\n",
    "    word_counts = Counter(words)\n",
    "    # vocab = [word for word,_ in Counter(words).most_common(vocab_size-1)]\n",
    "    # this is done to replace any word which is in the dataste but not within the top vocab_size.\n",
    "    pad_token = \".\"\n",
    "    unk_token = \"<UNK>\" \n",
    "    # this token replaces words that have a freq of <= 3\n",
    "    rare_token = \"<RARE>\"\n",
    "    # adding words only once\n",
    "    vocab=set()\n",
    "    rare_words_exist = False\n",
    "    # logic to add rare tokens. the following for loop will create a vocabulary for the model to use\n",
    "    for word, count in word_counts.items():\n",
    "        if count > 3:\n",
    "            # vocab.append(word)\n",
    "            vocab.add(word)\n",
    "        else:\n",
    "            # NOTE: we replace all words with <=3 freq with one token\n",
    "            rare_words_exist = True\n",
    "\n",
    "\n",
    "    vocab = [pad_token, unk_token] + list(vocab)\n",
    "\n",
    "    # remeber, the first three items in the vocab are : . , <UNK>, <RARE>\n",
    "    if rare_words_exist:\n",
    "        vocab.insert(2, rare_token)\n",
    "    \n",
    "    return vocab, word_counts\n",
    "\n",
    "def create_stoi_itos(vocab):\n",
    "    # mapping words to integer/count\n",
    "    stoi = {s: i + 1 for i, s in enumerate(vocab)}\n",
    "    # invert the dictionary\n",
    "    itos = {i: s for s, i in stoi.items()}\n",
    "    return stoi, itos\n",
    "\n",
    "def build_dataset(words, context_length, is_train=False, only_dict = False):\n",
    "    \"\"\"\n",
    "    words: dataset that contains a continuous stream of words (so basically like text from books for example)\n",
    "    context_length: how many words do you need to predict next one. we will be keeping this at 2\n",
    "    \"\"\"\n",
    "\n",
    "    vocab, word_counts = create_vocab(words)\n",
    "    stoi, itos = create_stoi_itos(vocab)\n",
    "    X, Y =[], []\n",
    "\n",
    "    # creating a context:target pair. this is basically input, output pairs. for example, if we are using a context of 2 and the next three words being iterated over are 'the quick fox' , X will be 'the quick' and Y (predicted) will be fox\n",
    "    context = [0]*context_length\n",
    "    for word in words:\n",
    "        # take the index of the word from stoi\n",
    "        # ix=stoi.get(word, stoi['<UNK>'])\n",
    "\n",
    "        if word_counts[word] > 3:\n",
    "             ix = stoi.get(word)\n",
    "        elif is_train and word_counts[word] <= 3:\n",
    "             ix = stoi.get(\"<RARE>\")\n",
    "        else:\n",
    "            ix = stoi[\"<UNK>\"]\n",
    "\n",
    "        # can use the below print for debugging purposes\n",
    "        # print(f\"word: {word}, its currrent context: {itos[context[0]]} {itos[context[1]]}\")\n",
    "\n",
    "        # create a list of words. so the current/target word and the words before it \n",
    "        # store the current context\n",
    "        X.append(context)\n",
    "        Y.append(ix)\n",
    "        \n",
    "        context = context[1:] + [ix]\n",
    "    \n",
    "    X = torch.tensor(X)\n",
    "    Y = torch.tensor(Y)\n",
    "\n",
    "    return X, Y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Splitting train test validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_test_split(train_ratio, words, context_length):\n",
    "    \"\"\"\n",
    "    train: percentage of dataset to be used for training the neural network\n",
    "    words: dataset that contains a list of words\n",
    "\n",
    "    returns training, testing and validation sets for the entered dataset \n",
    "    \"\"\"\n",
    "\n",
    "    test_split = 1 - ((1 - train_ratio)/2)\n",
    "    random.seed(42)\n",
    "\n",
    "    n1 = int(train_ratio*len(words))\n",
    "    n2 = int(test_split*len(words))\n",
    "    # print(words[:4])\n",
    "\n",
    "    # NOTE TO SELF: from the initial char-level model, i improved/changed by using one set of words for creating the vocab for all three splits. initially i was using different word sets for each and the inconsistency here was causing major issues. its a good point to take note of for later on.\n",
    "    \n",
    "    train_words = words[:n1]\n",
    "    dev_words = words[n1:n1+n2]\n",
    "    test_words = words[n1+n2:]\n",
    "\n",
    "    vocab, word_counts = create_vocab(train_words)\n",
    "    stoi, itos = create_stoi_itos(vocab)\n",
    "\n",
    "    data = build_dataset(words, context_length)\n",
    "\n",
    "    Xtr, Ytr = build_dataset(train_words, context_length, is_train=True)\n",
    "    Xdev, Ydev = build_dataset(dev_words, context_length, is_train=True)\n",
    "    Xte, Yte = build_dataset(test_words, context_length, is_train=True)\n",
    "\n",
    "    return [Xtr, Ytr, Xdev, Ydev, Xte, Yte, stoi, itos]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "brown_words = open(\"data/brown.txt\", \"r\").read().split(\" \")\n",
    "sets = train_test_split(0.8, brown_words, 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating the neural network parameters\n",
    "\n",
    "Ok before creating it i will briefly go through the model architecture that we are implementing from the paper. So the main model will consist of the input layer -> hidden layer -> output layer. \n",
    "\n",
    "however, (note this is optional as mentioned in the paper) the paper does go through another path. \n",
    "\n",
    "Basically, while the non-linear layer learns the complex relationship between words we can have another <b>linear</b> layer (which i refer to as a direct connection) that learns only the simple relationships. It connects the input layer's embeddings directly to the output layer. \n",
    "\n",
    "From the paper:\n",
    "\n",
    "<img src='images/direct_layer.png' >\n",
    "\n",
    "What this does is that it ensures that the model learns/captures both relationship types in order to perform well on new examples. the diagram below gives a better idea of the model design.\n",
    "\n",
    "<img src='images/model_design.png' >"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "def create_params(context_length, dims, hidden_layer_neurons, number_of_words):\n",
    "    \"\"\"\n",
    "    context_length: how many words do you need to predict next one\n",
    "\n",
    "    This function initializes paramters for a neural network with one hidden layer and the number of neurons within it are set by the user. \n",
    "    \"\"\"\n",
    "    # building lookup table C\n",
    "    # we will initalize the lookup table with random numbers\n",
    "    # generator for reproducibility of results\n",
    "    g = torch.Generator().manual_seed(1234567)\n",
    "\n",
    "    # here we squeeze the words within the specified dimensions\n",
    "    C = torch.rand((number_of_words, dims), generator=g)\n",
    "\n",
    "    # initialize hidden layer\n",
    "    # this will be 100 neurons  \n",
    "\n",
    "    # NOTE: the .uniform_ is a slight moving away from the paper and can be removed to maintain 100% alignment with the paper\n",
    "    W1 = torch.randn((context_length*dims, hidden_layer_neurons), generator=g).uniform_(-0.1, 0.1)\n",
    "    b1 = torch.randn(hidden_layer_neurons, generator=g)\n",
    "\n",
    "    # this was something i didnt realise earlier, after discussing with claude as well this is a layer that is a direct conenction between the feature vector and the final layer. this is a linear layer which is used to map connections between the most common words. \n",
    "    W = torch.randn((context_length*dims, number_of_words), generator=g).uniform_(-0.1, 0.1)\n",
    "\n",
    "    # creating output/final layer\n",
    "    W2 = torch.randn((hidden_layer_neurons, number_of_words), generator=g).uniform_(-0.1, 0.1)\n",
    "    b2 = torch.randn(number_of_words, generator=g)\n",
    "    parameters = [C, W1, b1, W2, b2, W]\n",
    "\n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(train_test_sets, parameters, context_length, dims, vocab, epochs):\n",
    "    C, W1, b1, W2, b2, W = parameters\n",
    "    Xtr, Ytr, Xdev, Ydev, Xte, Yte, stoi, itos = train_test_sets\n",
    "\n",
    "    # set grad calc to true for backprop\n",
    "    for p in parameters:\n",
    "        p.requires_grad = True\n",
    "    # training the neural network\n",
    "    lri = []\n",
    "    lossi = []\n",
    "\n",
    "    # Xtr, Ytr, Xdev, Ydev, Xte, Yte = train_test_sets\n",
    "\n",
    "    # print(\"After Mini_batch \\n\")\n",
    "    for epoch in range(epochs):\n",
    "\n",
    "        # mini-batch construct. \n",
    "        batch = 32\n",
    "        ix = torch.randint(0, Xtr.shape[0], (batch, ))\n",
    "        start_time = time.time()\n",
    "        # --------- forward pass --------- #\n",
    "        # embed all the inputs at once \n",
    "        # remember that when we do C[X], it treats each elemnt in X as an index to select a row in C\n",
    "\n",
    "        # print(f\"Xtr shape for embedding is: {Xtr.shape}\")\n",
    "\n",
    "        embed = C[Xtr[ix]]\n",
    "        # embed.view squashes 32, 3, 2 into 32, 6(3*2). using -1 avoids hardcoding the length and instead pytorch infers the length\n",
    "\n",
    "        # hidden layer calc\n",
    "        # h = torch.tanh(embed.view(-1, context_length*dims) @ W1 + b1)\n",
    "\n",
    "        # # direct connection/linear layer calculation\n",
    "        embed_flat = embed.view(batch, -1)\n",
    "\n",
    "        # now combine the non-linear layer and direct connection calc. u can consider it a second hidden layer. this, as mentioned in the paper, is optional. \n",
    "        # why ? non-linear layer learns the complex relationships and the linear layer calc learns the simple relationships\n",
    "        h = torch.tanh(embed_flat @ W1 + b1)\n",
    "        \n",
    "        # output layer calc\n",
    "        # logits = h @ W2 + b2\n",
    "        logits = (h @ W2) + (embed_flat @ W) + b2\n",
    "\n",
    "        # loss calc\n",
    "        loss = F.cross_entropy(logits, Ytr[ix])\n",
    "\n",
    "        if epoch%10==0:\n",
    "            print(f'Epoch: {epoch} | Loss: {loss.item()}')\n",
    "        # print(loss.item())\n",
    "\n",
    "        # ---------- backward pass ------- #\n",
    "\n",
    "        # set gradient for each param to 0 \n",
    "        for p in parameters:\n",
    "            p.grad = None\n",
    "        loss.backward()\n",
    "\n",
    "        # update parameter. learning rate\n",
    "        lr = 0.01\n",
    "        for p in parameters:\n",
    "            p.data += -lr * p.grad\n",
    "        \n",
    "        # update stats \n",
    "        lri.append(lr)\n",
    "        lossi.append(loss.item())\n",
    "        \n",
    "        end_time = time.time()\n",
    "        # print(f\"Iteration {epoch+1} took {end_time - start_time} seconds\\n\")\n",
    "    \n",
    "    return parameters, stoi, itos, Xdev, Ydev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_model(Xdev, Ydev, parameters):\n",
    "    C, W1, b1, W2, b2, W = parameters\n",
    "    # NOTE: MY initial code was computing gradients as well as extrmely slow. claude helped me add torch.no_grad() and batch processing for the validation set\n",
    "    total_loss = 0\n",
    "    num_batches = 0\n",
    "    batch_size = 32 \n",
    "\n",
    "    # stop recalculation of gradients for the params\n",
    "    with torch.no_grad():  \n",
    "        for i in range(0, len(Xdev), batch_size):\n",
    "            batch_X = Xdev[i:i+batch_size]\n",
    "            batch_Y = Ydev[i:i+batch_size]\n",
    "\n",
    "            # forward pass same as for training\n",
    "            embedData = C[batch_X]\n",
    "            embed_flat = embedData.view(embedData.size(0), -1)\n",
    "\n",
    "            actual_size = embedData.size(0)\n",
    "\n",
    "            h = torch.tanh(embed_flat @ W1 + b1)\n",
    "            logits = (h @ W2) + (embed_flat @ W) + b2\n",
    "            # print(logits)\n",
    "            loss = F.cross_entropy(logits, batch_Y)\n",
    "            \n",
    "            total_loss += loss.item()\n",
    "            num_batches += 1\n",
    "\n",
    "    average_loss = total_loss / num_batches\n",
    "    print(\"\\nAverage Validation Loss:\", average_loss)\n",
    "\n",
    "    # Optional: Calculate perplexity\n",
    "    perplexity = torch.exp(torch.tensor(average_loss))\n",
    "    print(\"Validation Perplexity:\", perplexity.item())\n",
    "\n",
    "    return average_loss, perplexity\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0 | Loss: 10.196393013000488\n",
      "Epoch: 10 | Loss: 10.427043914794922\n",
      "Epoch: 20 | Loss: 9.749485969543457\n",
      "Epoch: 30 | Loss: 7.904714584350586\n",
      "Epoch: 40 | Loss: 9.089534759521484\n",
      "Epoch: 50 | Loss: 9.225329399108887\n",
      "Epoch: 60 | Loss: 7.807784557342529\n",
      "Epoch: 70 | Loss: 8.79438591003418\n",
      "Epoch: 80 | Loss: 8.731880187988281\n",
      "Epoch: 90 | Loss: 8.786593437194824\n",
      "Epoch: 100 | Loss: 8.412059783935547\n",
      "Epoch: 110 | Loss: 9.00832748413086\n",
      "Epoch: 120 | Loss: 9.48662281036377\n",
      "Epoch: 130 | Loss: 8.198122024536133\n",
      "Epoch: 140 | Loss: 8.810020446777344\n",
      "Epoch: 150 | Loss: 7.967146873474121\n",
      "Epoch: 160 | Loss: 7.54957914352417\n",
      "Epoch: 170 | Loss: 7.411753177642822\n",
      "Epoch: 180 | Loss: 8.96130084991455\n",
      "Epoch: 190 | Loss: 7.531877517700195\n",
      "\n",
      "Average Validation Loss: 9.216295255039043\n",
      "Validation Perplexity: 10059.7265625\n",
      "Final Test Perplexity: 10059.73\n"
     ]
    }
   ],
   "source": [
    "# 1. Data Preparation\n",
    "brown_words = open(\"data/brown.txt\", \"r\").read().split(\" \")\n",
    "context_length = 2  # As used in the paper\n",
    "dims = 120  # Word feature vector dimensionality, as per paper\n",
    "\n",
    "# Split data and create vocabulary\n",
    "train_test_sets = train_test_split(0.8, brown_words, context_length)\n",
    "Xtr, Ytr, Xdev, Ydev, Xte, Yte, stoi, itos = train_test_sets\n",
    "vocab_size = len(stoi)\n",
    "\n",
    "# 2. Model Initialization\n",
    "hidden_layer_neurons = 200  # As per paper\n",
    "parameters = create_params(context_length, dims, hidden_layer_neurons, vocab_size)\n",
    "\n",
    "# 3. Training\n",
    "epochs = 200  # Paper mentions 10-20 epochs\n",
    "updated_params, trained_stoi, trained_itos, Xdev, Ydev = train_model(train_test_sets, parameters, context_length, dims, (stoi, itos), epochs)\n",
    "\n",
    "# 4. Final Evaluation (if not done in train_model)\n",
    "C, W1, b1, W2, b2, W = updated_params\n",
    "test_loss, test_perplexity = test_model(Xdev, Ydev, updated_params)\n",
    "print(f\"Final Test Perplexity: {test_perplexity:.2f}\")\n",
    "\n",
    "# 5. Save the model (optional)\n",
    "torch.save({\n",
    "    'model_params': updated_params,\n",
    "    'stoi': trained_stoi,\n",
    "    'itos': trained_itos\n",
    "}, 'neural_lm_model.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predicting the next word \n",
    "def predict_next_word(word1, word2, model_comps):\n",
    "    parameters, stoi, itos = model_comps\n",
    "    C, W1, b1, W2, b2, W = parameters\n",
    "\n",
    "    # converting the words to indices\n",
    "    ix1 = stoi.get(word1, stoi['<UNK>'])\n",
    "    ix2 = stoi.get(word2, stoi['<UNK>'])\n",
    "\n",
    "    input_indices = [ix1, ix2]\n",
    "    \n",
    "    # Convert to tensor\n",
    "    input_tensor = torch.tensor(input_indices).unsqueeze(0) \n",
    "    \n",
    "    embedData = C[input_tensor]\n",
    "    embed_flat = embedData.view(embedData.size(0), -1)\n",
    "    h = torch.tanh(embed_flat @ W1 + b1)\n",
    "    logits = (h @ W2) + (embed_flat @ W) + b2\n",
    "\n",
    "    preds = torch.nn.functional.softmax(logits)\n",
    "\n",
    "    # Get the top 5 predictions\n",
    "    top_preds, top_indices = torch.topk(preds, 5)\n",
    "    \n",
    "    print(\"Top 5 predictions:\")\n",
    "    for prob, idx in zip(top_preds[0], top_indices[0]):\n",
    "        print(f\"{itos[idx.item()]}: {prob.item():.4f}\")\n",
    "\n",
    "    # print(preds)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 5 predictions:\n",
      "<RARE>: 0.1099\n",
      "the: 0.0502\n",
      "of: 0.0328\n",
      "and: 0.0223\n",
      "a: 0.0091\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/cz/sb2kbc4925qcf91d0v0pj8ch0000gn/T/ipykernel_45644/2033465049.py:20: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  preds = torch.nn.functional.softmax(logits)\n"
     ]
    }
   ],
   "source": [
    "loaded_data = torch.load('neural_lm_model.pth')\n",
    "model_params = loaded_data['model_params']\n",
    "stoi= loaded_data['stoi']\n",
    "itos=loaded_data['itos']\n",
    "\n",
    "predict_next_word(word1=\"are\", word2='you', model_comps=[model_params, stoi, itos])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mlpenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
